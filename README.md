# Dynamic ORB based In-MSCKF
**Authors:** [Gitesh Sambhaji Gunjal](https://github.com/giteshgunjal), [Ming-Yang Lo](https://github.com/MingYangLo), [Ruichang Chen](https://github.com/chenrc98), [Chengyu Shi](https://github.com/chengyus1012)

This is the ccode repostitory for Team 2 of the Winter 2022 version of Mocile Robotics. 
This project made three major changes to the [MSCKF_VIO](https://github.com/KumarRobotics/msckf_vio). First, the feature extractor changed to ORB. Second, implemented [YOLACT](https://github.com/dbolya/yolact) with [pretrained weights](*link) for object instance segmentation to create masks of predicted dynamic objects. Last, modified the propagation, augmentation, and correction steps of [MSCKF_VIO](https://github.com/KumarRobotics/msckf_vio)from EKF to RightIn-EKF.

## License

Dynamic ORB based In-MSCKF is based on [MSCKF_VIO](https://github.com/KumarRobotics/msckf_vio) and [YOLACT](https://github.com/dbolya/yolact), and as such are released under a [Penn Software License](https://github.com/KumarRobotics/msckf_vio/blob/master/LICENSE.txt) and [MIT License](https://github.com/dbolya/yolact/blob/master/LICENSE) respectively.

## Dependencies
This software is tested on Ubuntu 18.04 with Melodic & Ubuntu 20.04 with ROS Noetic.

- For MSCKF_VIO
    Dependencies are standard including `Eigen`, `OpenCV`, and `Boost`. One special requirement is `suitesparse`, which can be installed through,
    ```
   sudo apt-get install libsuitesparse-dev
    ```
- For YOLACT
    Set up the environment using the follwin methosd:
    - Setup a Python3 environment
    - Install Pytorch 1.0.1 (or higher) and TorchVision
    - Install some other packages:
    ```
    pip install cython
    pip install opencv-python pillow pycocotools matplotlib
    ```
    To use YOLACT with ROS, we need to build the cv_bridge with Python3 using the following command:
    ```
    # install dependencies
    sudo apt-get install python-catkin-tools python3-dev python3-catkin-pkg-modules python3-numpy python3-yaml ros-kinetic-cv-bridge
    # create ROS workspace
    mkdir catkin_workspace
    # init ROS workspace
    catkin init
    # set cmake variables, you may need to change the python version to your system version
    catkin config -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.5m -DPYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so
    catkin config --install
    # Clone cv_bridge src
    git clone https://github.com/ros-perception/vision_opencv.git src/vision_opencv
    # Find version of cv_bridge in your repository
    apt-cache show ros-kinetic-cv-bridge | grep Version
        Version: 1.12.8-0xenial-20180416-143935-0800
    # Checkout right version in git repo. In our case it is 1.12.8
    cd src/vision_opencv/
    git checkout 1.12.8
    cd ../../
    # Build
    catkin build cv_bridge
    # Extend environment with new package
    source install/setup.bash --extend
    ```


## Compiling and Installation
For YOLACT, download the yolact_resnet50_54_800000.pth model from [YOLACT repo](https://github.com/dbolya/yolact) and put it in weights folder

While compiling with Ubuntu 18.04 or 20.04, should include the [random_numbers folder](https://github.com/ros-planning/random_numbers) which is also included [in this repository](https://github.com/MingYangLo/Mobile_Robotics/tree/main/Mobile_Robotics_Final_Project/random_numbers) for a successful compilation.

The software is a standard catkin package. Make sure the package is on ROS_PACKAGE_PATH after cloning the package to your workspace. And the normal procedure for compiling a catkin package should work.
```
cd your_work_space
catkin_make --pkg msckf_vio --cmake-args -DCMAKE_BUILD_TYPE=Release
```

## Calibration
An accurate calibration is crucial for successfully running the software. To get the best performance of the software, the stereo cameras and IMU should be hardware synchronized. Note that for the stereo calibration, which includes the camera intrinsics, distortion, and extrinsics between the two cameras, you have to use a calibration software. **Manually setting these parameters will not be accurate enough.** [Kalibr](https://github.com/ethz-asl/kalibr) can be used for the stereo calibration and also to get the transformation between the stereo cameras and IMU. The yaml file generated by Kalibr can be directly used in this software. See calibration files in the `config` folder for details. The two calibration files in the `config` folder should work directly with the EuRoC and KAIST (only set for urban28, might need a further look if running on others) datasets. The convention of the calibration file is as follows:

`camx/T_cam_imu`: takes a vector from the IMU frame to the camx frame.
`cam1/T_cn_cnm1`: takes a vector from the cam0 frame to the cam1 frame.

The filter uses the first 200 IMU messages to initialize the gyro bias, acc bias, and initial orientation. Therefore, the robot is required to start from a stationary state in order to initialize the VIO successfully.


## EuRoC and KAIST dataset example usage

First obtain either the [EuRoC](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets) or the [KAIST Urban](https://sites.google.com/view/complex-urban-dataset/download-lidar-stereo?authuser=0) dataset.

Recommended EuRoC ROS Bags:
- [Vicon Room 1 01](http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_01_easy/V1_01_easy.bag)
- [Vicon Room 1 02](http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.bag)
- [Vicon Room 1 03](http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_03_difficult/V1_03_difficult.bag)

Once the `msckf_vio` is built and sourced (via `source <path to catkin_ws>/devel/setup.bash`), there are launch files prepared for the [EuRoC](https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets) and [KAIST Urban](https://sites.google.com/view/complex-urban-dataset/download-lidar-stereo?authuser=0) dataset named `msckf_vio_euroc.launch` and `msckf_vio_kaist.launch` respectively. Each launch files instantiates two ROS nodes:

* `image_processor` processes stereo images to detect and track features
* `vio` obtains feature measurements from the `image_processor` and tightly fuses them with the IMU messages to estimate pose.

These launch files can be executed via

```
roslaunch msckf_vio msckf_vio_euroc.launch
```
or

```
roslaunch msckf_vio msckf_vio_kaist.launch
```

To start YOLACT node, open a terminal at YOLACT folder and run:
```
python3 ros_yolact.py --trained_model=weights/yolact_resnet50-54-800000.pth --score_threshold=0.15 --top_k=15
```

Once the nodes are running you need to run the dataset rosbags (in a different terminal), for example:

```
rosbag play V1_01_easy.bag
```

As mentioned in the previous section, **The robot is required to start from a stationary state in order to initialize the VIO successfully.**

To visualize the pose and feature estimates you can use the provided rviz configurations found in `msckf_vio/rviz` folder (EuRoC: `rviz_euroc_config.rviz`, Fast dataset: `rviz_kaist_config.rviz`).

Note: 

1. If show `online reset` while running the system, please restart all process. 

2. KAIST dataset might not work that well due to few dataset mathematical issues.


## ROS Nodes

### `image_processor` node

**Subscribed Topics**

`imu` (`sensor_msgs/Imu`)

IMU messages is used for compensating rotation in feature tracking, and 2-point RANSAC.

`cam[x]_image` (`sensor_msgs/Image`)

Synchronized stereo images.

**Published Topics**

`features` (`msckf_vio/CameraMeasurement`)

Records the feature measurements on the current stereo image pair.

`tracking_info` (`msckf_vio/TrackingInfo`)

Records the feature tracking status for debugging purpose.

`debug_stereo_img` (`sensor_msgs::Image`)

Draw current features on the stereo images for debugging purpose. Note that this debugging image is only generated upon subscription.

### `vio` node

**Subscribed Topics**

`imu` (`sensor_msgs/Imu`)

IMU measurements.

`features` (`msckf_vio/CameraMeasurement`)

Stereo feature measurements from the `image_processor` node.

**Published Topics**

`odom` (`nav_msgs/Odometry`)

Odometry of the IMU frame including a proper covariance.

`feature_point_cloud` (`sensor_msgs/PointCloud2`)

Shows current features in the map which is used for estimation.


